{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1fef93a-7119-4f75-aa1c-78aa7ac9e321",
   "metadata": {},
   "source": [
    "# Magic BERT\n",
    "#### Investigating the similarity of MtG flavor text with a large language model\n",
    "\n",
    "We will be using the [DistilBERT](https://huggingface.co/docs/transformers/en/model_doc/distilbert) model from HuggingFace's Transformers package. DistilBERT is a relatively lightweight transformer model that still performs well at most benchmarking tasks. \n",
    "\n",
    "This notebook is based off of the [NLP Course](https://huggingface.co/learn/nlp-course/en) from HuggingFace and [this Geeks for Geeks post](https://www.geeksforgeeks.org/sentence-similarity-using-bert-transformer/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c849ea33-c366-416e-952c-e021399b5ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Create Model Object\n",
    "from transformers import DistilBertModel\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0639d953-bf4a-438f-bb80-185fa7ee9ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load card data\n",
    "import pickle\n",
    "datapath = 'card_data/CardData.pkl'\n",
    "with open(datapath,'rb') as file:    \n",
    "    data = pickle.load(file)\n",
    "data = data.loc[:,['name','type_line','set','mana_cost','colors','flavor_text']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4240123-506b-416d-b1a6-4c1c38a1fa91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Spray :\n",
      "it's the only kind of shower goblins will tolerate.\n"
     ]
    }
   ],
   "source": [
    "# Test out tokenizing some text\n",
    "test_flavor = data.loc[42,'flavor_text']\n",
    "print(data.loc[42,'name'])\n",
    "print(test_flavor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "61af87c6-d9a5-4676-a33a-abd5d47e2138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', \"'\", 's', 'the', 'only', 'kind', 'of', 'shower', 'goblin', '##s', 'will', 'tolerate', '.']\n",
      "[2009, 1005, 1055, 1996, 2069, 2785, 1997, 6457, 22639, 2015, 2097, 19242, 1012]\n"
     ]
    }
   ],
   "source": [
    "# Use HuggingFace's AutoTokenizer to select the tokenizer used for our pre-trained DistilBERT model\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "tokens = tokenizer.tokenize(test_flavor)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ac8902b0-e111-44bc-9d44-bbe7f7d74cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2009,  1005,  1055,  1996,  2069,  2785,  1997,  6457, 22639,\n",
      "          2015,  2097, 19242,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Format as a multidimensional tensor for use by the transformer\n",
    "tokenized_inputs = tokenizer(test_flavor, return_tensors=\"pt\")\n",
    "print(tokenized_inputs)\n",
    "print(type(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "796e5d62-ab6f-41cd-9f43-7907370ecda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "{'input_ids': tensor([[  101,  2009,  1005,  1055,  1996,  2069,  2785,  1997,  6457, 22639,\n",
      "          2015,  2097, 19242,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "BaseModelOutput(last_hidden_state=tensor([[[ 0.0616,  0.2137, -0.0266,  ..., -0.0677,  0.1408,  0.3962],\n",
      "         [ 0.0283, -0.0310, -0.1482,  ...,  0.1277, -0.0508,  0.3766],\n",
      "         [ 0.1092,  0.1527,  0.4453,  ..., -0.1133, -0.0174,  0.0917],\n",
      "         ...,\n",
      "         [ 0.1770,  0.6853,  0.3198,  ...,  0.1025, -0.4718,  0.3971],\n",
      "         [ 0.8807,  0.3451, -0.3388,  ...,  0.0228, -0.5681, -0.4490],\n",
      "         [ 0.8425,  0.7682,  0.2190,  ..., -0.0927, -0.4687, -0.0884]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n",
      "<class 'transformers.modeling_outputs.BaseModelOutput'>\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(test_flavor, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(type(tokens))\n",
    "print(tokens)\n",
    "output = model(**tokens)\n",
    "print(output)\n",
    "print(type(output))\n",
    "# print(output.last_hidden_state[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "27775757-5535-4515-ac1d-f1073842096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write two similar sentences and measure their similarity:\n",
    "sentence1 = \"Goblins sure do smell bad.\"\n",
    "sentence2 = \"I'm glad I don't smell as bad as a goblin!\"\n",
    "sequences = [sentence1,sentence2]\n",
    "tokens = tokenizer(sequences,padding=True,return_tensors='pt')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "61d4b2a4-b241-45b8-b0f7-dc6f2aad0b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    out1 = outputs.last_hidden_state[0,0,:]\n",
    "    out2 = outputs.last_hidden_state[1,0,:]\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b44f585a-1006-438e-bce1-eff0873d12ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9646025]]\n",
      "[[0.9646025  0.99999994]\n",
      " [1.0000002  0.9646025 ]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity([out1],[out2]))\n",
    "print(cosine_similarity([out1,out2],[out2,out1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e5b8fe97-c7d6-4598-98de-bd0c10d9507d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1.reshape(1,-1).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
