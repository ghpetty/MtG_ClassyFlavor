{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1fef93a-7119-4f75-aa1c-78aa7ac9e321",
   "metadata": {},
   "source": [
    "# Magic BERT\n",
    "#### Investigating the similarity of MtG flavor text with a large language model\n",
    "\n",
    "We will be using the [DistilBERT](https://huggingface.co/docs/transformers/en/model_doc/distilbert) model from HuggingFace's Transformers package. DistilBERT is a relatively lightweight transformer model that still performs well at most benchmarking tasks. \n",
    "\n",
    "This notebook is based off of the [NLP Course](https://huggingface.co/learn/nlp-course/en) from HuggingFace and [this Geeks for Geeks post](https://www.geeksforgeeks.org/sentence-similarity-using-bert-transformer/).\n",
    "\n",
    "WORK IN PROGRESS\n",
    "See this sentence transformer documentation for next steps:\n",
    "https://huggingface.co/sentence-transformers/msmarco-distilbert-base-v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c849ea33-c366-416e-952c-e021399b5ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Create Model Object\n",
    "from transformers import DistilBertModel\n",
    "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0639d953-bf4a-438f-bb80-185fa7ee9ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load card data\n",
    "import pickle\n",
    "datapath = 'card_data/CardData.pkl'\n",
    "with open(datapath,'rb') as file:    \n",
    "    data = pickle.load(file)\n",
    "data = data.loc[:,['name','type_line','set','mana_cost','colors','flavor_text']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4240123-506b-416d-b1a6-4c1c38a1fa91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Spray :\n",
      "it's the only kind of shower goblins will tolerate.\n"
     ]
    }
   ],
   "source": [
    "# Test out tokenizing some text\n",
    "test_flavor = data.loc[42,'flavor_text']\n",
    "print(data.loc[42,'name'])\n",
    "print(test_flavor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "61af87c6-d9a5-4676-a33a-abd5d47e2138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['it', \"'\", 's', 'the', 'only', 'kind', 'of', 'shower', 'goblin', '##s', 'will', 'tolerate', '.']\n",
      "[2009, 1005, 1055, 1996, 2069, 2785, 1997, 6457, 22639, 2015, 2097, 19242, 1012]\n"
     ]
    }
   ],
   "source": [
    "# Use HuggingFace's AutoTokenizer to select the tokenizer used for our pre-trained DistilBERT model\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "tokens = tokenizer.tokenize(test_flavor)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ac8902b0-e111-44bc-9d44-bbe7f7d74cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2009,  1005,  1055,  1996,  2069,  2785,  1997,  6457, 22639,\n",
      "          2015,  2097, 19242,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Format as a multidimensional tensor for use by the transformer\n",
    "tokenized_inputs = tokenizer(test_flavor, return_tensors=\"pt\")\n",
    "print(tokenized_inputs)\n",
    "print(type(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "796e5d62-ab6f-41cd-9f43-7907370ecda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "{'input_ids': tensor([[  101,  2009,  1005,  1055,  1996,  2069,  2785,  1997,  6457, 22639,\n",
      "          2015,  2097, 19242,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "BaseModelOutput(last_hidden_state=tensor([[[ 0.0616,  0.2137, -0.0266,  ..., -0.0677,  0.1408,  0.3962],\n",
      "         [ 0.0283, -0.0310, -0.1482,  ...,  0.1277, -0.0508,  0.3766],\n",
      "         [ 0.1092,  0.1527,  0.4453,  ..., -0.1133, -0.0174,  0.0917],\n",
      "         ...,\n",
      "         [ 0.1770,  0.6853,  0.3198,  ...,  0.1025, -0.4718,  0.3971],\n",
      "         [ 0.8807,  0.3451, -0.3388,  ...,  0.0228, -0.5681, -0.4490],\n",
      "         [ 0.8425,  0.7682,  0.2190,  ..., -0.0927, -0.4687, -0.0884]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), hidden_states=None, attentions=None)\n",
      "<class 'transformers.modeling_outputs.BaseModelOutput'>\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(test_flavor, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(type(tokens))\n",
    "print(tokens)\n",
    "output = model(**tokens)\n",
    "print(output)\n",
    "print(type(output))\n",
    "# print(output.last_hidden_state[:,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "27775757-5535-4515-ac1d-f1073842096c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 22639,  2015,  2469,  2079,  5437,  2919,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  1049,  5580,  1045,  2123,  1005,  1056,  5437,\n",
      "          2004,  2919,  2004,  1037, 22639,   999,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# Write two similar sentences and measure their similarity:\n",
    "sentence1 = \"Goblins sure do smell bad.\"\n",
    "sentence2 = \"I'm glad I don't smell as bad as a goblin!\"\n",
    "sequences = [sentence1,sentence2]\n",
    "tokens = tokenizer(sequences,padding=True,return_tensors='pt')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "61d4b2a4-b241-45b8-b0f7-dc6f2aad0b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score = [0.9646025]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    out1 = outputs.last_hidden_state[0,0,:]\n",
    "    out2 = outputs.last_hidden_state[1,0,:]\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# cosine_similarity needs 2D arrays, so reshape our vectors accordingly:\n",
    "out1 = out1.reshape(1,-1)\n",
    "out2 = out2.reshape(1,-1)\n",
    "similarity_score = cosine_similarity(out1,out2)\n",
    "print(f\"Similarity score = {similarity_score[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ba97e1e5-e2b2-43e6-8ddd-2dae92770f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 22639,  2015,  2469,  2079,  5437,  2919,  1012,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1045,  1005,  1049,  5580,  1045,  2123,  1005,  1056,  5437,\n",
      "          2004,  2919,  2004,  1037, 22639,   999,   102],\n",
      "        [  101,  1996,  3224,  2003,  1996,  2087,  3376,  4487, 16643,  3363,\n",
      "          3672,  1997,  3267,  1005,  1055, 17284,   102],\n",
      "        [  101,  5082,  2003, 16158,  2006,  1996,  2019, 14762,  1997,  3255,\n",
      "           102,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "# Now let's compare some dissimilar statements:\n",
    "sentence3 = \"The forest is the most beautiful distillment of nature's bounty\"\n",
    "sentence4 = \"Progress is forged on the anvil of pain\"\n",
    "sequences = [sentence1,sentence2,sentence3,sentence4]\n",
    "tokens = tokenizer(sequences,padding=True,return_tensors='pt')\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9514f861-2234-486e-9180-3ac32e1def94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score = [0.9646025]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    out3 = outputs.last_hidden_state[0,0,:].reshape(1,-1)\n",
    "    out4 = outputs.last_hidden_state[1,0,:].reshape(1,-1)\n",
    "similarity_score = cosine_similarity(out3,out4)\n",
    "print(f\"Similarity score = {similarity_score[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e5b8fe97-c7d6-4598-98de-bd0c10d9507d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999994, 0.9646025 , 0.88829386, 0.9054321 ],\n",
       "       [0.9646025 , 1.0000002 , 0.8974154 , 0.9165001 ],\n",
       "       [0.88829386, 0.8974154 , 1.        , 0.9296369 ],\n",
       "       [0.9054321 , 0.9165001 , 0.9296369 , 0.9999999 ]], dtype=float32)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(outputs.last_hidden_state[:,0,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
